{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY2LNNB2cBDZ"
      },
      "outputs": [],
      "source": [
        "! pip3 install -U bitsandbytes\n",
        "! pip3 install code-lens@git+https://github.com/cisnlp/code-lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4VUK7k2cBDb"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McVtmD8pcBDc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Change to your own gpu ids\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbHt9yUEcBDc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "# fix random seed\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfekJt1-cBDc"
      },
      "outputs": [],
      "source": [
        "from code_lens import LlamaHelper\n",
        "from code_lens import generate_heatmap\n",
        "from code_lens import visualize_heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cddUYpTicBDd"
      },
      "outputs": [],
      "source": [
        "# Change to your own token, model, and cache path\n",
        "\n",
        "hf_token = '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'\n",
        "custom_model = \"codellama/CodeLlama-7b-hf\"\n",
        "cache_directory = './transformers_cache/'\n",
        "load_in_8bit = True # False\n",
        "\n",
        "if custom_model is not None:\n",
        "    model = LlamaHelper(dir=custom_model, load_in_8bit=load_in_8bit, hf_token=hf_token,cache_directory=cache_directory)\n",
        "    tokenizer = model.tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pApKbDZIcBDd"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJWrj1w9cBDd"
      },
      "outputs": [],
      "source": [
        "java_snippets = [\n",
        "    'String message = \"Hello\";',\n",
        "    'public class MyClass {}',\n",
        "    'public int value = 5;',\n",
        "    'public void doSomething() {}',\n",
        "    'int result = add(3, 5);',\n",
        "    'for (int i = 0; i < 10; i++)',\n",
        "    'if (x > 5) { /* ... */ }',\n",
        "    'try { /* ... */ } catch (Exception e) { /* ... */ }',\n",
        "    'System.out.println(\"Hello\");'\n",
        "]\n",
        "\n",
        "rust_snippets = [\n",
        "    'let message = \"Hello\";',\n",
        "    'struct MyClass {}',\n",
        "    'let value: i32 = 5;',\n",
        "    'fn do_something() {}',\n",
        "    'let result = add(3, 5);',\n",
        "    'for i in 0..10',\n",
        "    'if x > 5 { /* ... */ }',\n",
        "    'match result { Ok(value) => { /* ... */ }, Err(e) => { /* ... */ } }',\n",
        "    'println!(\"Hello\");'\n",
        "]\n",
        "\n",
        "# Initialize an empty prompt string\n",
        "prompt = \"\"\n",
        "\n",
        "# Loop through both lists and add each Java-Rust pair to the prompt\n",
        "for java, rust in zip(java_snippets, rust_snippets):\n",
        "    prompt += f'Java: {java} - Rust: {rust}\\n'\n",
        "\n",
        "prompt = prompt.strip()\n",
        "\n",
        "# Print the merged result\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_beams = 10\n",
        "max_length = 1\n",
        "\n",
        "layers = list(range(1, 31))\n",
        "\n",
        "min_position = 123\n",
        "max_position = 134\n",
        "\n",
        "heatmap_data = generate_heatmap(model=model,\n",
        "                                tokenizer=tokenizer,\n",
        "                                device=device,\n",
        "                                text=prompt,\n",
        "                                layers=layers,\n",
        "                                num_beams=num_beams,\n",
        "                                max_length=max_length,\n",
        "                                min_position=min_position,\n",
        "                                max_position=max_position)"
      ],
      "metadata": {
        "id": "AIrm-ciIQP19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCIe2EyOcBDd"
      },
      "outputs": [],
      "source": [
        "num_beams = 3\n",
        "max_length = 2\n",
        "\n",
        "layers = [10, 20, 21, 25, 31]\n",
        "min_position = 123\n",
        "max_position = 134\n",
        "\n",
        "heatmap_data = generate_heatmap(model=model,\n",
        "                                tokenizer=tokenizer,\n",
        "                                device=device,\n",
        "                                text=prompt,\n",
        "                                layers=layers,\n",
        "                                num_beams=num_beams,\n",
        "                                max_length=max_length,\n",
        "                                min_position=min_position,\n",
        "                                max_position=max_position)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGq1N_3AcBDe"
      },
      "outputs": [],
      "source": [
        "layers_to_show = heatmap_data['layers']\n",
        "token_indices_to_show = range(min(min_position, len(heatmap_data['tokens'])), max(max_position, 0))\n",
        "\n",
        "visualize_heatmap(heatmap_data,\n",
        "                  layers_to_show,\n",
        "                  token_indices_to_show,\n",
        "                  trunc_size = 6)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}