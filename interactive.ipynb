{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install code-lens@git+https://github.com/cisnlp/code-lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Change to your own gpu ids\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# fix random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_lens import LlamaHelper\n",
    "from code_lens import generate_heatmap\n",
    "from code_lens import visualize_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your own token, model, and cache path\n",
    "\n",
    "hf_token = 'hf_XxxxXXXxxXxxXXxxxxXXxXXxxXxXXxxxxX'\n",
    "custom_model = \"codellama/CodeLlama-7b-hf\"\n",
    "cache_directory = './transformers_cache/'\n",
    "load_in_8bit = True # False\n",
    "\n",
    "if custom_model is not None:\n",
    "    model = LlamaHelper(dir=custom_model, load_in_8bit=load_in_8bit, hf_token=hf_token,cache_directory=cache_directory)\n",
    "    tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_snippets = [\n",
    "    'String message = \"Hello\";',\n",
    "    'public class MyClass {}',\n",
    "    'public int value = 5;',\n",
    "    'public void doSomething() {}',\n",
    "    'int result = add(3, 5);',\n",
    "    'for (int i = 0; i < 10; i++)',\n",
    "    'if (x > 5) { /* ... */ }',\n",
    "    'try { /* ... */ } catch (Exception e) { /* ... */ }',\n",
    "    'System.out.println(\"Hello\");'\n",
    "]\n",
    "\n",
    "rust_snippets = [\n",
    "    'let message = \"Hello\";',\n",
    "    'struct MyClass {}',\n",
    "    'let value: i32 = 5;',\n",
    "    'fn do_something() {}',\n",
    "    'let result = add(3, 5);',\n",
    "    'for i in 0..10',\n",
    "    'if x > 5 { /* ... */ }',\n",
    "    'match result { Ok(value) => { /* ... */ }, Err(e) => { /* ... */ } }',\n",
    "    'println!(\"Hello\");'\n",
    "]\n",
    "\n",
    "# Initialize an empty prompt string\n",
    "prompt = \"\"\n",
    "\n",
    "# Loop through both lists and add each Java-Rust pair to the prompt\n",
    "for java, rust in zip(java_snippets, rust_snippets):\n",
    "    prompt += f'Java: {java} - Rust: {rust}\\n'\n",
    "\n",
    "prompt = prompt.strip()\n",
    "\n",
    "# Print the merged result\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beams = 3\n",
    "max_length = 2\n",
    "\n",
    "layers = [10, 20, 21, 25, 31]\n",
    "min_position = 123\n",
    "max_position = 134\n",
    "\n",
    "heatmap_data = generate_heatmap(model=model, tokenizer=tokenizer, device=device, text=prompt, layers = layers, num_beams=num_beams, max_length=max_length, min_position = min_position, max_position = max_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_show = heatmap_data['layers']\n",
    "token_indices_to_show = range(min(min_position, len(heatmap_data['tokens'])), max(max_position, 0))\n",
    "visualize_heatmap(heatmap_data, layers_to_show, token_indices_to_show, trunc_size = 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
