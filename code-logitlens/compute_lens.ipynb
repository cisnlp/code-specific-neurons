{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = 'C#'\n",
    "targets = ['PHP', 'Java', 'Javascript', 'Python', 'C++', 'C', 'C#']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4VUK7k2cBDb"
   },
   "source": [
    "### Import and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McVtmD8pcBDc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change to your own gpu ids\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbHt9yUEcBDc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# fix random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfekJt1-cBDc"
   },
   "outputs": [],
   "source": [
    "from .llamawrapper import LlamaHelper\n",
    "from .utils import generate_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cddUYpTicBDd"
   },
   "outputs": [],
   "source": [
    "# Change to your own token, model, and cache path\n",
    "\n",
    "hf_token = \"hf_xxXxXxXXXXxxxxxXXxxxxxXXXXXXXxXXxx\"\n",
    "# custom_model = \"codellama/CodeLlama-7b-hf\"\n",
    "custom_model = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "cache_directory = './transformers_cache/'\n",
    "load_in_8bit = False\n",
    "\n",
    "if custom_model is not None:\n",
    "    model = LlamaHelper(dir=custom_model, device=device, load_in_8bit=load_in_8bit, hf_token=hf_token,cache_directory=cache_directory)\n",
    "    tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pApKbDZIcBDd"
   },
   "source": [
    "### Compute Lens for Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beams = 10\n",
    "max_length = 1\n",
    "layers = list(range(0, 32))\n",
    "\n",
    "parallel_path = '../datasets/parallel/code_snippets'\n",
    "prompt_path = '../datasets/parallel/prompts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(lang, postfix):\n",
    "    return f'{prompt_path}/{lang.lower()}_{postfix}.txt'\n",
    "\n",
    "def find_last_index(lst, value):\n",
    "    return len(lst) - 1 - lst[::-1].index(value)\n",
    "\n",
    "def save(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(source_snippets, target_snippets, source_lang, target_lang):\n",
    "    \n",
    "    # Initialize an empty prompt string\n",
    "    prompt = \"\"\n",
    "\n",
    "    # Loop through both lists and add each Java-Rust pair to the prompt\n",
    "    for s, t in zip(source_snippets, target_snippets):\n",
    "        prompt += f'{source_lang}: {s} - {target_lang}: {t}\\n'\n",
    "\n",
    "    prompt = prompt.strip()\n",
    "\n",
    "    # Return the merged result\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def test(intext, soruce_text):\n",
    "    \n",
    "    min_position = len(tokenizer.tokenize(soruce_text))\n",
    "    max_position = len(tokenizer.tokenize(intext))\n",
    "    \n",
    "        \n",
    "    heatmap_data = generate_heatmap(model=model,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    device=device,\n",
    "                                    text=intext,\n",
    "                                    layers=layers,\n",
    "                                    num_beams=num_beams,\n",
    "                                    max_length=max_length,\n",
    "                                    min_position=min_position,\n",
    "                                    max_position=max_position,\n",
    "                                    batch_size=1)\n",
    "\n",
    "    return heatmap_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "max_num_parallel_sent = 100\n",
    "\n",
    "source_path = get_path(source_lang, 'fewshot')\n",
    "\n",
    "# Load the file to verify\n",
    "with open(source_path, 'r') as file:\n",
    "    source_snippets = [l.strip() for l in file.readlines()]\n",
    "\n",
    "for target_lang in targets:\n",
    "    \n",
    "    if target_lang == source_lang:\n",
    "        continue\n",
    "        \n",
    "    target_path = get_path(target_lang, 'fewshot')\n",
    "\n",
    "    with open(target_path, 'r') as file:\n",
    "        target_snippets = [l.strip() for l in file.readlines()]\n",
    "    \n",
    "    \n",
    "    output_path = f'./outputs-{custom_model.replace('/', '-')}/{source_lang.lower()}-{target_lang.lower()}'\n",
    "\n",
    "    # Check if the directory exists, create it if it doesn't\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    \n",
    "    p = prompt(source_snippets, target_snippets, source_lang, target_lang)\n",
    "    \n",
    "    \n",
    "    for file_id in tqdm(os.listdir(f'{parallel_path}/{source_lang}')[:max_num_parallel_sent]):\n",
    "        \n",
    "        if os.path.exists(os.path.join(output_path, file_id)):\n",
    "            continue\n",
    "        \n",
    "        source_test = [json.load(open(f'{parallel_path}/{source_lang}/{file_id}'))['snippet'].strip()]\n",
    "        target_test = [json.load(open(f'{parallel_path}/{target_lang}/{file_id}'))['snippet'].strip()]\n",
    "        p_test = prompt(source_test, target_test, source_lang, target_lang)\n",
    "\n",
    "        \n",
    "        intext = p + '\\n' + p_test\n",
    "\n",
    "        \n",
    "        last_index = intext.rfind(target_lang) + len(target_lang + ':') + 1\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                heatmap_data = test(intext, intext[:last_index])\n",
    "\n",
    "            save(heatmap_data, os.path.join(output_path, file_id))\n",
    "            del heatmap_data\n",
    "\n",
    "        except:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            gc.collect()\n",
    "        \n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
