{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copyright: https://github.com/cisnlp/MEXA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a798e8",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a3c6c2",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "token = \"hf_xxXxXxXXXXxxxxxXXxxxxxXXXXXXXxXXxx\"\n",
    "\n",
    "model_path = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', cache_dir = '../transformers_cache/', use_auth_token=token)                                                         \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_auth_token=token)\n",
    "tokenizer.pad_token =  tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def weighted_embeddings(layer, attention_mask, device='cuda'):\n",
    "    \n",
    "    # Compute the weights for non-padding tokens\n",
    "    weights_for_non_padding = attention_mask * torch.arange(start=1, end=layer.shape[1] + 1, device=device).unsqueeze(0)\n",
    "\n",
    "    # Sum the embeddings weighted by non-padding tokens\n",
    "    sum_embeddings = torch.sum(layer * weights_for_non_padding.unsqueeze(-1), dim=1)\n",
    "    num_of_none_padding_tokens = torch.sum(weights_for_non_padding, dim=-1).unsqueeze(-1)\n",
    "    \n",
    "    # Compute the sentence embeddings\n",
    "    sentence_embeddings = sum_embeddings / num_of_none_padding_tokens\n",
    "    sentence_embeddings = sentence_embeddings.squeeze().cpu().numpy()\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83936637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_layers(text,device='cuda'):\n",
    "\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True).to(device)\n",
    "\n",
    "\n",
    "    sentence_embeddings_weighted = []\n",
    "    sentence_embeddings_last_token = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Forward pass through the model\n",
    "        hidden_state_layers = model(**tokens, output_hidden_states=True)[\"hidden_states\"]\n",
    "\n",
    "        for layer in hidden_state_layers:\n",
    "\n",
    "            # Ensure attention mask is on the same device\n",
    "            attention_mask = tokens.attention_mask.to(device)\n",
    "            # Only use the last token in the attention mask\n",
    "            attention_mask_last = torch.zeros_like(attention_mask).to(device)\n",
    "            attention_mask_last[:, -1] = 1\n",
    "\n",
    "            embd_weighted = weighted_embeddings(layer, attention_mask, device)\n",
    "            embd_last_token = weighted_embeddings(layer, attention_mask_last, device)\n",
    "\n",
    "            sentence_embeddings_weighted.append(embd_weighted)\n",
    "            sentence_embeddings_last_token.append(embd_last_token)\n",
    "\n",
    "    return sentence_embeddings_weighted, sentence_embeddings_last_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed2526",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Directory where the files are located\n",
    "directory = '../datasets/parallel/code_snippets/'\n",
    "\n",
    "# Initialize an empty dictionary to store results\n",
    "result_dict = {}\n",
    "\n",
    "\n",
    "all_langs = ['C', 'C++', 'C#', 'Java', 'Javascript', 'PHP', 'Python']\n",
    "\n",
    "all_filenames = os.listdir(os.path.join(directory, all_langs[0]))\n",
    "# Iterate through the files in the input directory\n",
    "for lang in all_langs:\n",
    "    sentences = []\n",
    "    for idx, file_name in enumerate(all_filenames):    \n",
    "        \n",
    "        file_path = os.path.join((os.path.join(directory, lang)), file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "            for line in infile:\n",
    "                entry = json.loads(line)\n",
    "                sentences.append({'id': idx + 1, 'text': entry['snippet']})\n",
    "    \n",
    "    result_dict[lang] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for idx, file_name in enumerate(all_filenames):    \n",
    "\n",
    "    file_path = os.path.join((os.path.join(directory, 'C')), file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            entry = json.loads(line)\n",
    "            sentences.append({'id': idx + 1, 'text': entry['comment']})\n",
    "\n",
    "    result_dict['eng_Latn'] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "os.makedirs(f\"./embd-{model_path.replace('/', '-')}\", exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "\n",
    "# Extract embeddings -- limit to top_text\n",
    "for language, texts in tqdm(result_dict.items()):\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    for text in texts:\n",
    "\n",
    "        embds_weighted, embds_last_token = get_embedding_layers(text['text'])\n",
    "\n",
    "        for layer in range(len(embds_weighted)):\n",
    "            \n",
    "            if layer not in embeddings_dict:\n",
    "                embeddings_dict[layer] = []\n",
    "\n",
    "            embeddings_dict[layer].append({'id': text['id'], 'embd_wighted': embds_weighted[layer], 'embd_last_token': embds_last_token[layer]})\n",
    "\n",
    "\n",
    "    with open(f\"./embd-{model_path.replace('/', '-')}/{language}.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(embeddings_dict, pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed42209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "import argparse\n",
    "\n",
    "def cosine_similarity(array1, array2):\n",
    "    cosine_dist = cosine(array1, array2)\n",
    "    cosine_similarity = 1 - cosine_dist\n",
    "    return cosine_similarity\n",
    "\n",
    "def mexa(matrix):\n",
    "    n = len(matrix)  # size of the square matrix\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Get the diagonal element\n",
    "        diag_element = matrix[i][i]\n",
    "        \n",
    "        # Get the row and column\n",
    "        row = matrix[i]\n",
    "        column = matrix[:,i]\n",
    "        \n",
    "        # Check if the diagonal element is strictly greater than all other elements in its row (excluding itself)\n",
    "        if diag_element > max(np.delete(row, i)):\n",
    "            # Check if the diagonal element is strictly greater than all other elements in its column (excluding itself)\n",
    "            if diag_element > max(np.delete(column, i)):\n",
    "                count += 1\n",
    "\n",
    "    # Normalized count\n",
    "    count_norm = count / n\n",
    "    return count_norm\n",
    "\n",
    "def compute_distance(lang, embedding_type='embd_weighted', num_sents=100):\n",
    "    with open(os.path.join(embedding_path, f\"{lang}.pkl\"), \"rb\") as pickle_file:\n",
    "        lang_embd = pickle.load(pickle_file)    \n",
    "\n",
    "    similarities_dict = {}\n",
    "    for layer in lang_embd.keys():\n",
    "        pivot_embd_layer = pivot_embd[layer][:num_sents]\n",
    "        lang_embd_layer = lang_embd[layer][:num_sents]\n",
    "        \n",
    "        # Initialize the similarities_dict matrix for each layer\n",
    "        num_actual_sentences = min(len(pivot_embd_layer), len(lang_embd_layer))\n",
    "        similarities_dict[layer] = np.zeros((num_actual_sentences, num_actual_sentences))\n",
    "        \n",
    "        # Compute similarities\n",
    "        for p_id, pivot_single in enumerate(pivot_embd_layer):\n",
    "            for l_id, lang_single in enumerate(lang_embd_layer):\n",
    "                similarities_dict[layer][p_id, l_id] = cosine_similarity(pivot_single[embedding_type], lang_single[embedding_type])\n",
    "\n",
    "    alignments = {}\n",
    "    for layer in lang_embd.keys():\n",
    "        alignments[layer] = mexa(similarities_dict[layer])\n",
    "    \n",
    "    return alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fbcced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_path = f\"./embd-{model_path.replace('/', '-')}/\"\n",
    "\n",
    "embedding_type = 'embd_wighted'\n",
    "\n",
    "num_sents = 100\n",
    "\n",
    "\n",
    "all_langs = ['C', 'C++', 'C#', 'Java', 'Javascript', 'PHP', 'Python', 'eng_Latn']\n",
    "\n",
    "for latent_lang in all_langs:\n",
    "    \n",
    "    save_path = f\"mexa-{latent_lang}-{model_path.replace('/', '-')}\"\n",
    "\n",
    "    # Load the pivot embeddings\n",
    "    with open(os.path.join(embedding_path, f'{latent_lang}.pkl'), \"rb\") as pickle_file:\n",
    "        pivot_embd = pickle.load(pickle_file)\n",
    "\n",
    "    languages = [filename[:-len('.pkl')] for filename in os.listdir(embedding_path) if filename.endswith('.pkl')]\n",
    "\n",
    "    for lang in tqdm(languages):\n",
    "        alignment_lang = compute_distance(lang, embedding_type=embedding_type, num_sents=num_sents)\n",
    "        save_filepath = os.path.join(save_path, f\"{lang}.json\")\n",
    "        os.makedirs(os.path.dirname(save_filepath), exist_ok=True)\n",
    "\n",
    "        with open(save_filepath, \"w\") as json_file:\n",
    "            json.dump(alignment_lang, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb80508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "os.makedirs(f\"./mexa-{model_path.replace('/', '-')}-figures\", exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# List of languages\n",
    "all_langs = ['C', 'C++', 'C#', 'Java', 'Javascript', 'PHP', 'Python',]\n",
    "\n",
    "# Loop over each folder\n",
    "for latent_lang in all_langs:\n",
    "    input_path = f\"mexa-{latent_lang}-{model_path.replace('/', '-')}\"\n",
    "\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"Folder {input_path} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f\"Data in Folder: {input_path}\")\n",
    "    plt.xlabel(\"Indices\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.ylim(0, 1)\n",
    "    # Loop over each language to read the JSON files\n",
    "    for lang in all_langs:\n",
    "        json_file_path = os.path.join(input_path, f\"{lang}.json\")\n",
    "\n",
    "        # Check if the JSON file exists\n",
    "        if not os.path.exists(json_file_path):\n",
    "            print(f\"File {json_file_path} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        # Read the JSON file\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Extract x and y values\n",
    "        x = list(map(int, data.keys()))\n",
    "        y = list(data.values())\n",
    "\n",
    "        # Plot the data with a unique color for each language\n",
    "        plt.plot(x, y, label=lang)\n",
    "\n",
    "    plt.legend(title=\"Languages\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(f\"./mexa-{model_path.replace('/', '-')}-figures/{input_path}_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Plot saved for folder {input_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d0502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of languages\n",
    "all_langs = ['C', 'C++', 'C#', 'Java', 'Javascript', 'PHP', 'Python']\n",
    "\n",
    "# Create a big figure for subplots\n",
    "fig, axes = plt.subplots(nrows=len(all_langs), ncols=1, figsize=(10, 6 * len(all_langs)), sharex=True)\n",
    "fig.tight_layout(pad=4.0)\n",
    "fig.suptitle(\"All Language Data Plots\", fontsize=16)\n",
    "\n",
    "# Loop over each folder\n",
    "for i, latent_lang in enumerate(all_langs):\n",
    "    input_path = f\"mexa-{latent_lang}-{model_path.replace('/', '-')}\"\n",
    "    ax = axes[i] if len(all_langs) > 1 else axes  # Handle case with one subplot\n",
    "\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"Folder {input_path} does not exist.\")\n",
    "        ax.set_title(f\"Folder {input_path} does not exist.\")\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "\n",
    "    ax.set_title(f\"Data in Folder: {input_path}\")\n",
    "    ax.set_xlabel(\"Indices\")\n",
    "    ax.set_ylabel(\"Values\")\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    # Loop over each language to read the JSON files\n",
    "    for lang in all_langs:\n",
    "        json_file_path = os.path.join(input_path, f\"{lang}.json\")\n",
    "\n",
    "        # Check if the JSON file exists\n",
    "        if not os.path.exists(json_file_path):\n",
    "            print(f\"File {json_file_path} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        # Read the JSON file\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Extract x and y values\n",
    "        x = list(map(int, data.keys()))\n",
    "        y = list(data.values())\n",
    "\n",
    "        # Plot the data with a unique color for each language\n",
    "        ax.plot(x, y, label=lang)\n",
    "\n",
    "    ax.legend(title=\"Languages\")\n",
    "    ax.grid(True)\n",
    "\n",
    "# Save the combined figure\n",
    "os.makedirs(f\"./mexa-{model_path.replace('/', '-')}-figures\", exist_ok=True)\n",
    "plt.savefig(f\"./mexa-{model_path.replace('/', '-')}-figures/all_languages_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Combined plot saved as all_languages_plot.png.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c539add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of languages\n",
    "all_langs = ['C#', 'Java', 'C', 'C++', 'Javascript', 'PHP', 'Python']\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.title(\"Accumulated Results for Each Latent Language\")\n",
    "plt.xlabel(\"Layers\")\n",
    "plt.ylabel(\"Alignment Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "\n",
    "markers = ['o', 's', 'D', '^', 'v', 'p', '*', 'X']\n",
    "marker_idx = 0  # Index to track the marker to use\n",
    "\n",
    "# Loop over each latent language\n",
    "for latent_lang in all_langs:\n",
    "    input_path = f\"mexa-{latent_lang}-{model_path.replace('/', '-')}\"\n",
    "\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"Folder {input_path} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    accumulated_values = {}\n",
    "\n",
    "    # Loop over each language file\n",
    "    for lang in all_langs:\n",
    "        json_file_path = os.path.join(input_path, f\"{lang}.json\")\n",
    "\n",
    "        # Check if the JSON file exists\n",
    "        if not os.path.exists(json_file_path):\n",
    "            print(f\"File {json_file_path} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        # Read the JSON file\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Accumulate values\n",
    "        for key, value in data.items():\n",
    "            key = int(key)\n",
    "            accumulated_values[key] = accumulated_values.get(key, 0) + value\n",
    "\n",
    "    if accumulated_values:\n",
    "        # Extract x and y values for plotting\n",
    "        x = sorted(accumulated_values.keys())\n",
    "        y = [accumulated_values[k] / len(all_langs) for k in x]  # Average the values\n",
    "        \n",
    "        print(latent_lang, sum(y))\n",
    "        \n",
    "        marker = markers[marker_idx % len(markers)]\n",
    "        marker_idx += 1\n",
    "        # Plot the accumulated results for the current latent language\n",
    "        plt.ylim(0.4, 1)\n",
    "        plt.plot(x, y, label=latent_lang, marker=marker, linestyle='-', markersize=4, alpha=0.7)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(title=\"PLs\", loc='lower right')\n",
    "plt.savefig(f\"./mexa-{model_path.replace('/', '-')}-figures/mexa-{model_path.replace('/', '-')}.pdf\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Accumulated results plot saved as accumulated_results_plot.png.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
