{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copyright: https://github.com/RUCAIBox/Language-Specific-Neurons/blob/main/activation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc5ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['HF_TOKEN'] = \"hf_xxXxXxXXXXxxxxxXXxxxxxXXXXXXXxXXxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a614ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f828e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = {'model': \"codellama/CodeLlama-7b-hf\", 'lang': 'sql'}\n",
    "args = {'model': \"meta-llama/Llama-3.1-8B\", 'lang': 'c#'}\n",
    "\n",
    "is_llama = bool(args['model'].lower().find('llama') >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c786d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLM(model=args['model'], tensor_parallel_size=torch.cuda.device_count(), enforce_eager=True, download_dir = '../transformers_cache')\n",
    "# model = LLM('../transformers_cache/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/', tensor_parallel_size=torch.cuda.device_count(), enforce_eager=True, download_dir = '../transformers_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e01241",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.llm_engine.model_config.max_model_len = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aca98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = model.llm_engine.model_config.max_model_len\n",
    "num_layers = model.llm_engine.model_config.hf_config.num_hidden_layers\n",
    "intermediate_size = model.llm_engine.model_config.hf_config.intermediate_size if is_llama else model.llm_engine.model_config.hf_config.hidden_size * 4\n",
    "\n",
    "over_zero = torch.zeros(num_layers, intermediate_size, dtype=torch.int32).to('cuda')\n",
    "\n",
    "def factory(idx):\n",
    "    def llama_forward(self, x):\n",
    "        gate_up, _ = self.gate_up_proj(x)  # b, l, 2i\n",
    "        i = gate_up.size(-1)\n",
    "        gate_up[:, :, : i // 2] = torch.nn.SiLU()(gate_up[:, :, : i // 2])\n",
    "        activation = gate_up[:, :, : i // 2].float() # b, l, i\n",
    "        over_zero[idx, :] += (activation > 0).sum(dim=(0,1))\n",
    "        x = gate_up[:, :, : i // 2] * gate_up[:, :, i // 2 :]\n",
    "        x, _ = self.down_proj(x)\n",
    "        return x\n",
    "\n",
    "    def bloom_forward(self, x: torch.Tensor):\n",
    "        x, _ = self.dense_h_to_4h(x)\n",
    "        x = self.gelu_impl(x)\n",
    "        activation = x.float()\n",
    "        over_zero[idx, :] += (activation > 0).sum(dim=(0,1))\n",
    "        x, _ = self.dense_4h_to_h(x)\n",
    "        return x\n",
    "\n",
    "    if is_llama:\n",
    "        return llama_forward\n",
    "    else:\n",
    "        return bloom_forward\n",
    "\n",
    "for i in range(num_layers):\n",
    "    if is_llama:\n",
    "        obj = model.llm_engine.driver_worker.model_runner.model.model.layers[i].mlp\n",
    "    else:\n",
    "        obj = model.llm_engine.driver_worker.model_runner.model.transformer.h[i].mlp\n",
    "    obj.forward = MethodType(factory(i), obj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994f960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = args['lang']\n",
    "\n",
    "if is_llama:\n",
    "    ids = torch.load(f'data/id.{lang}.train.llama')\n",
    "else:\n",
    "    ids = torch.load(f'data/id.{lang}.train.bloom')\n",
    "l = ids.size(0)\n",
    "l = min(l, 99999744) // max_length * max_length\n",
    "input_ids = ids[:l].reshape(-1, max_length)\n",
    "\n",
    "output = model.generate(prompt_token_ids=input_ids.tolist(), sampling_params=SamplingParams(max_tokens=1))\n",
    "\n",
    "output = dict(n=l, over_zero=over_zero.to('cpu'))\n",
    "\n",
    "if is_llama:\n",
    "    torch.save(output, f'data/activation.{lang}.train.llama')\n",
    "else:\n",
    "    torch.save(output, f'data/activation.{lang}.train.bloom')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
