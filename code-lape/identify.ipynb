{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138bcfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['c','c++', 'c#', 'java', 'rust', 'python', 'javascript', 'php', 'html', 'go', 'ruby', 'wiki']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62907a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, over_zero = [], []\n",
    "for lang in languages:\n",
    "    data = torch.load(f'data/activation.{lang}.train.llama')\n",
    "    n.append(data['n'])\n",
    "    over_zero.append(data['over_zero'])\n",
    "\n",
    "n = torch.tensor(n)\n",
    "over_zero = torch.stack(over_zero, dim=-1)\n",
    "\n",
    "num_layers, intermediate_size, lang_num = over_zero.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(f\"./activation_mask\", exist_ok=True)  # Create the directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12183828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_activation(top_rate = 0.01, filter_rate = 0.95, activation_bar_ratio = 0.95):\n",
    "\n",
    "    activation_probs = over_zero / n # layer x inter x lang_num\n",
    "    normed_activation_probs = activation_probs / activation_probs.sum(dim=-1, keepdim=True)\n",
    "    normed_activation_probs[torch.isnan(normed_activation_probs)] = 0\n",
    "    log_probs = torch.where(normed_activation_probs > 0, normed_activation_probs.log(), 0)\n",
    "    entropy = - torch.sum(normed_activation_probs * log_probs, dim=-1)\n",
    "    largest = False\n",
    "    \n",
    "    if torch.isnan(entropy).sum():\n",
    "        print(torch.isnan(entropy).sum())\n",
    "        raise ValueError\n",
    "    \n",
    "    flattened_probs = activation_probs.flatten()\n",
    "    top_prob_value = flattened_probs.kthvalue(round(len(flattened_probs) * filter_rate)).values.item()\n",
    "    print(top_prob_value)\n",
    "    # dismiss the neruon if no language has an activation value over top 90%\n",
    "    top_position = (activation_probs > top_prob_value).sum(dim=-1)\n",
    "    entropy[top_position == 0] = -torch.inf if largest else torch.inf\n",
    "\n",
    "    flattened_entropy = entropy.flatten()\n",
    "    top_entropy_value = round(len(flattened_entropy) * top_rate)\n",
    "    print(flattened_entropy)\n",
    "    _, index = flattened_entropy.topk(top_entropy_value, largest=largest)\n",
    "    row_index = index // entropy.size(1)\n",
    "    col_index = index % entropy.size(1)\n",
    "    selected_probs = activation_probs[row_index, col_index] # n x lang\n",
    "    print(selected_probs)\n",
    "    # for r, c in zip(row_index, col_index):\n",
    "    #     print(r, c, activation_probs[r][c])\n",
    "\n",
    "    print(selected_probs.size(0), torch.bincount(selected_probs.argmax(dim=-1)))\n",
    "    selected_probs = selected_probs.transpose(0, 1)\n",
    "    activation_bar = flattened_probs.kthvalue(round(len(flattened_probs) * activation_bar_ratio)).values.item()\n",
    "    print((selected_probs > activation_bar).sum(dim=1).tolist())\n",
    "    lang, indice = torch.where(selected_probs > activation_bar)\n",
    "    print(lang, indice)\n",
    "    merged_index = torch.stack((row_index, col_index), dim=-1)\n",
    "    final_indice = []\n",
    "    for _, index in enumerate(indice.split(torch.bincount(lang).tolist())):\n",
    "        lang_index = [tuple(row.tolist()) for row in merged_index[index]]\n",
    "        lang_index.sort()\n",
    "        layer_index = [[] for _ in range(num_layers)]\n",
    "        for l, h in lang_index:\n",
    "            layer_index[l].append(h)\n",
    "        for l, h in enumerate(layer_index):\n",
    "            layer_index[l] = torch.tensor(h).long()\n",
    "        final_indice.append(layer_index)\n",
    "    torch.save(final_indice, f\"activation_mask/top-{top_rate}-filter-{filter_rate}-activation-{activation_bar_ratio}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_activation(filter_rate = 0.95, activation_bar_ratio = 0.95, num_neurons_per_lang = 400):\n",
    "    activation_probs = over_zero / n # layer x inter x lang_num\n",
    "    normed_activation_probs = activation_probs / activation_probs.sum(dim=-1, keepdim=True)\n",
    "    normed_activation_probs[torch.isnan(normed_activation_probs)] = 0\n",
    "    log_probs = torch.where(normed_activation_probs > 0, normed_activation_probs.log(), 0)\n",
    "    entropy = - torch.sum(normed_activation_probs * log_probs, dim=-1)\n",
    "    largest = False\n",
    "    \n",
    "    if torch.isnan(entropy).sum():\n",
    "        print(torch.isnan(entropy).sum())\n",
    "        raise ValueError\n",
    "    \n",
    "    flattened_probs = activation_probs.flatten()\n",
    "    top_prob_value = flattened_probs.kthvalue(round(len(flattened_probs) * filter_rate)).values.item()\n",
    "    print(top_prob_value)\n",
    "    # dismiss the neruon if no language has an activation value over top 90%\n",
    "    top_position = (activation_probs > top_prob_value).sum(dim=-1)\n",
    "    entropy[top_position == 0] = -torch.inf if largest else torch.inf\n",
    "\n",
    "    flattened_entropy = entropy.flatten()\n",
    "#     top_entropy_value = round(len(flattened_entropy) * top_rate)\n",
    "    print(flattened_entropy)\n",
    "#     _, index = flattened_entropy.topk(top_entropy_value, largest=largest)\n",
    "#     row_index = index // entropy.size(1)\n",
    "#     col_index = index % entropy.size(1)\n",
    "#     selected_probs = activation_probs[row_index, col_index] # n x lang\n",
    "#     print(selected_probs)\n",
    "    # for r, c in zip(row_index, col_index):\n",
    "    #     print(r, c, activation_probs[r][c])\n",
    "\n",
    "#     print(selected_probs.size(0), torch.bincount(selected_probs.argmax(dim=-1)))\n",
    "#     selected_probs = selected_probs.transpose(0, 1)\n",
    "    activation_bar = flattened_probs.kthvalue(round(len(flattened_probs) * activation_bar_ratio)).values.item()\n",
    "\n",
    "    # Corrected torch.where usage\n",
    "    layer_idx, neuron_idx, lang_idx = torch.where(activation_probs > activation_bar)\n",
    "    \n",
    "    # Merge row and column indices\n",
    "    merged_index = torch.stack((layer_idx, neuron_idx), dim=-1)  # (num_selected, 2)\n",
    "\n",
    "    # Organize neurons per language\n",
    "    final_indice = []\n",
    "    unique_languages = torch.unique(lang_idx)\n",
    "\n",
    "    for l in unique_languages:\n",
    "        # Get all indices for the current language\n",
    "        lang_indices = merged_index[lang_idx == l]\n",
    "        \n",
    "        # Sort neurons by entropy (low to high)\n",
    "        lang_entropies = entropy[lang_indices[:, 0], lang_indices[:, 1]]\n",
    "        sorted_indices = torch.argsort(lang_entropies)\n",
    "\n",
    "        # Select the top `num_neurons_per_lang` neurons\n",
    "        selected_indices = lang_indices[sorted_indices[:num_neurons_per_lang]]\n",
    "\n",
    "        # Convert to (layer, neuron) tuples\n",
    "        lang_index = [tuple(row.tolist()) for row in selected_indices]\n",
    "        \n",
    "        # Organize indices per layer\n",
    "        layer_index = [[] for _ in range(num_layers)]\n",
    "        for layer, neuron in lang_index:\n",
    "            layer_index[layer].append(neuron)\n",
    "        \n",
    "        # Convert lists to tensors\n",
    "        for layer, neurons in enumerate(layer_index):\n",
    "            layer_index[layer] = torch.tensor(neurons).long()\n",
    "        \n",
    "        final_indice.append(layer_index)\n",
    "\n",
    "    # Save results (commented out for now)\n",
    "    torch.save(final_indice, f\"activation_mask/neurons-{num_neurons_per_lang}-filter-{filter_rate}-activation-{activation_bar_ratio}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ffebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_activation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
